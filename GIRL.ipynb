{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXdeHDmEPQM6"
   },
   "source": [
    "## G-learing and GIRL for wealth optimization\n",
    "\n",
    "This notebook demonstrates the application of G-learning and GIRL for optimization of a defined contribution retirement plan. The notebook extends the G-learning notebook in Chapter 10 with an example of applying GIRL to infer the parameters of the G-learner used to generate the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOoE16hWPQNC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M-6MadDnPQNO",
    "outputId": "176e0a88-271e-4be2-9d69-1f13e1520534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0+cpu'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHyhUTpJ4nl1",
    "outputId": "18b1c78b-d6c6-4453-9755-65390364ab53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KMP_DUPLICATE_LIB_OK=TRUE\n"
     ]
    }
   ],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0yKeMU9PQNa"
   },
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLivr9yuPQON"
   },
   "outputs": [],
   "source": [
    "# Define the G-learning portfolio optimization class\n",
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 params,\n",
    "                 beta,\n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky assets\n",
    "                 init_x_vals, # array of initial asset position values (num_risky_assets + 1)\n",
    "                 use_for_WM = True): # use for wealth management tasks\n",
    "\n",
    "                \n",
    "        self.num_steps = num_steps\n",
    "        self.num_assets = num_risky_assets + 1 # exp_returns.shape[1]\n",
    "        \n",
    "        self.lambd = torch.tensor(params[0], requires_grad=False, dtype=torch.float64)\n",
    "        self.Omega_mat = params[1] * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        #self.Omega_mat = torch.tensor(Omega_mat,requires_grad=False, dtype=torch.float64)\n",
    "        self.eta = torch.tensor(params[2], requires_grad=False, dtype=torch.float64)\n",
    "        self.rho = torch.tensor(params[3], requires_grad=False, dtype=torch.float64)\n",
    "        self.beta = torch.tensor(beta, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_risky_assets = num_risky_assets\n",
    "        self.r_f = riskfree_rate\n",
    "        \n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets\n",
    "        \n",
    "        self.Sigma_r_np = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        self.reg_mat = 1e-3*torch.eye(self.num_assets, dtype=torch.float64)\n",
    "        \n",
    "        # arrays of returns for all assets including the risk-free asset\n",
    "        # array of shape num_steps x (num_stocks + 1) \n",
    "        self.exp_returns_np = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))\n",
    "                                      \n",
    "        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside\n",
    "        self.Sigma_r_tilde_np = np.zeros((self.num_assets, self.num_assets))\n",
    "        self.Sigma_r_tilde_np[1:,1:] = self.Sigma_r_np\n",
    "            \n",
    "        # make Torch tensors  \n",
    "        self.exp_returns = torch.tensor(self.exp_returns_np,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r = torch.tensor(Sigma_r,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r_tilde = torch.tensor(self.Sigma_r_tilde_np,requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.benchmark_portf = torch.tensor(benchmark_portf, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        # asset holding values for all times. Initialize with initial values, \n",
    "        # values for the future times will be expected values \n",
    "        self.x_vals_np = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.x_vals_np[0,:] = init_x_vals \n",
    "        \n",
    "        # Torch tensor\n",
    "        self.x_vals = torch.tensor(self.x_vals_np)\n",
    "                \n",
    "        # allocate memory for coefficients of R-, F- and G-functions        \n",
    "        self.F_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets, dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.F_x = torch.zeros(self.num_steps, self.num_assets, dtype=torch.float64,\n",
    "                               requires_grad=True)\n",
    "        self.F_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.Q_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.R_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "        \n",
    "        self.reset_prior_policy()\n",
    "        \n",
    "        # the list of adjustable model parameters:\n",
    "        self.model_params = [self.lambd, self.beta, self.Omega_mat, self.eta]  \n",
    "#                              self.exp_returns, self.Sigma_r_tilde,self.Sigma_prior_inv, self.u_bar_prior]\n",
    "        \n",
    "        \n",
    "        # expected cash installment for all steps\n",
    "        self.expected_c_t = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # realized values of the target portfolio\n",
    "        self.realized_target_portf = np.zeros(self.num_steps,dtype=np.float64)\n",
    "        \n",
    "        # expected portfolio values for all times\n",
    "        self.expected_portf_val = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # the first value is the sum of initial position values\n",
    "        self.expected_portf_val[0] = self.x_vals[0,:].sum()\n",
    "\n",
    "    def reset_prior_policy(self):\n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = torch.zeros(self.num_steps,self.num_assets,requires_grad=False,\n",
    "                                       dtype=torch.float64)\n",
    "        self.v_bar_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior_inv = torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior_inv[t,:,:] = 10.0 * torch.eye(self.num_assets).clone() # np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "    \n",
    "    def reward_fun(self, t, x_vals, u_vals, exp_rets, lambd, Sigma_hat):\n",
    "        \"\"\"\n",
    "        The reward function \n",
    "        \"\"\"\n",
    "        x_plus = x_vals + u_vals\n",
    "        \n",
    "        p_hat = self.rho.clone() * self.benchmark_portf[t] + (1-self.rho.clone())*self.eta.clone()*x_vals.sum()\n",
    "        \n",
    "        aux_1 = - self.lambd.clone() * p_hat**2         \n",
    "        aux_2 = - u_vals.sum()   \n",
    "        aux_3 = 2*self.lambd.clone() * p_hat * x_plus.dot(torch.ones(num_assets) + exp_rets)\n",
    "        aux_4 = - self.lambd.clone() * x_plus.mm(Sigma_hat.mv(x_plus))\n",
    "        aux_5 = - u_vals.mm(self.Omega_mat.clone().mv(u_vals))\n",
    "        \n",
    "        return aux_1 + aux_2 + aux_3 + aux_4 + aux_5  \n",
    "    \n",
    "    def compute_reward_fun(self):\n",
    "        \"\"\"\n",
    "        Compute coefficients R_xx, R_ux, etc. for all steps\n",
    "        \"\"\"\n",
    "        for t in range(0, self.num_steps):\n",
    "            \n",
    "            one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "            benchmark_portf = self.benchmark_portf[t]\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            self.R_xx[t,:,:] = (-self.lambd.clone()*(self.eta.clone()**2)*(self.rho.clone()**2)*one_one_T_mat\n",
    "                                 + 2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_ux[t,:,:] = (2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - 2*self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_uu[t,:,:] = - self.lambd.clone() * Sigma_hat - self.Omega_mat.clone()\n",
    "            \n",
    "            self.R_x[t,:] =  (-2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*(1-self.rho.clone())*benchmark_portf *\n",
    "                                 torch.ones(self.num_assets,dtype=torch.float64)\n",
    "                                 + 2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret)\n",
    "            \n",
    "            self.R_u[t,:] = (2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret\n",
    "                             - torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            \n",
    "            self.R_0[t] = - self.lambd.clone()*((1-self.rho.clone())**2) * (benchmark_portf**2)\n",
    "                \n",
    "         \n",
    "    def project_cash_injections(self):\n",
    "        \"\"\"\n",
    "        Compute the expected values of future asset positions, and the expected cash injection for future steps,\n",
    "        as well as realized values of the target portfolio\n",
    "        \"\"\"\n",
    "           \n",
    "        # this assumes that the policy is trained\n",
    "        for t in range(1, self.num_steps):  # the initial value is fixed \n",
    "            \n",
    "            # increment the previous x_t\n",
    "            \n",
    "            delta_x_t = self.u_bar_prior[t,:] + self.v_bar_prior[t,:,:].mv(self.x_vals[t-1,:])\n",
    "            self.x_vals[t,:] = self.x_vals[t-1,:] + delta_x_t\n",
    "            \n",
    "            # grow using the expected return\n",
    "            self.x_vals[t,:] = (torch.ones(self.num_assets)+ self.exp_returns[t,:])*self.x_vals[t,:]\n",
    "            \n",
    "            # compute c_t\n",
    "            self.expected_c_t[t] = delta_x_t.sum().data # detach().numpy()\n",
    "            \n",
    "            # expected portfolio value for this step\n",
    "            self.expected_portf_val[t] = self.x_vals[t,:].sum().data # .detach().numpy()\n",
    "    \n",
    "            \n",
    "                                                                                      \n",
    "    def set_terminal_conditions(self):\n",
    "        \"\"\"\n",
    "        set the terminal condition for the F-function\n",
    "        \"\"\"\n",
    "        \n",
    "        # the auxiliary quantity to perform matrix calculations\n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[-1,:]\n",
    "        \n",
    "        \n",
    "        # Compute the reward function for all steps (only the last step is needed for this functions, while \n",
    "        # values for other time steps will be used in other functions)\n",
    "        self.compute_reward_fun()\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            Sigma_hat_inv = torch.inverse(Sigma_hat + self.reg_mat)\n",
    "            \n",
    "            Sigma_tilde = Sigma_hat + (1/self.lambd)*self.Omega_mat.clone()\n",
    "            #Sigma_tilde_inv = torch.pinverse(Sigma_tilde)\n",
    "            Sigma_tilde_inv = torch.inverse(Sigma_tilde + self.reg_mat)\n",
    "            \n",
    "            Sigma_hat_sigma_tilde = Sigma_hat.mm(Sigma_tilde)\n",
    "            Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.mm(Sigma_hat)\n",
    "            Sigma_tilde_sigma_hat = Sigma_tilde.mm(Sigma_hat)\n",
    "            \n",
    "            Sigma_hat_Sigma_tilde_inv = Sigma_hat.mm(Sigma_tilde_inv)\n",
    "            Sigma_3_plus_omega = self.lambd*Sigma_tilde_inv.mm(Sigma_hat_Sigma_tilde_inv) + self.Omega_mat.clone()    \n",
    "                             \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            Sigma_tilde_inv_t_R_ux = Sigma_tilde_inv.t().mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.t().mm(self.R_uu[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_u = Sigma_tilde_inv.t().mv(self.R_u[-1,:].clone())\n",
    "            \n",
    "            Sigma_tilde_inv_R_u = Sigma_tilde_inv.mv(self.R_u[-1,:].clone())\n",
    "            Sigma_tilde_inv_R_ux = Sigma_tilde_inv.mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.mm(self.R_uu[-1,:,:].clone())\n",
    "            \n",
    "            # though the action at the last step is deterministic, we can feed \n",
    "            # parameters of the prior with these values                     \n",
    "              \n",
    "            self.u_bar_prior[-1,:]   = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mv(self.R_u[-1,:].clone())\n",
    "            self.v_bar_prior[-1,:,:] = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())    \n",
    "                \n",
    "            # First compute the coefficients of the reward function at the last step\n",
    "\n",
    "            \n",
    "            # the coefficients of F-function for the last step\n",
    "            \n",
    "            # F_xx                 \n",
    "            self.F_xx[-1,:,:] = (self.R_xx[-1,:,:].clone()\n",
    "                                 + (1/(2*self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mm(Sigma_tilde_inv_t_R_ux)\n",
    "                                 + (1/(4*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mm(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mm(Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())))\n",
    "                                )\n",
    "            \n",
    "            # F_x                    \n",
    "            self.F_x[-1,:] = (self.R_x[-1,:].clone()\n",
    "                                 + (1/(self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mv(Sigma_tilde_inv_t_R_u.clone())\n",
    "                                 + (1/(2*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mv(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                            )\n",
    "            \n",
    "            \n",
    "        \n",
    "            # F_0   \n",
    "            self.F_0[-1] = (self.R_0[-1].clone() \n",
    "                            +  (1/(2*self.lambd.clone()))* self.R_u[-1,:].clone().dot(Sigma_tilde_inv_R_u.clone())\n",
    "                            + (1/(4*self.lambd.clone()**2))* self.R_u[-1,:].clone().dot(\n",
    "                                Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                           )\n",
    "            \n",
    "            # for the Q-function at the last step:\n",
    "            self.Q_xx[-1,:,:] = self.R_xx[-1,:,:].clone()\n",
    "            self.Q_ux[-1,:,:] = self.R_ux[-1,:,:].clone()\n",
    "            self.Q_uu[-1,:,:] = self.R_uu[-1,:,:].clone()\n",
    "            self.Q_u[-1,:] = self.R_u[-1,:].clone()\n",
    "            self.Q_x[-1,:] = self.R_x[-1,:].clone()\n",
    "            self.Q_0[-1] = self.R_0[-1].clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"   \n",
    "        print('Doing G-learning, it may take a few seconds...')\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # make matrix Sigma_hat_t        \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "        Sigma_hat_t = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "        \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = torch.diag(torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:])\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].clone()  \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].clone()     \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = torch.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*torch.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "            tol = torch.max(err_u_bar, err_v_bar)  # tol = 0.5*(err_u_bar + err_v_bar)\n",
    "            \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "                \n",
    "            \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "                \n",
    "        ones = torch.ones(self.num_assets,dtype=torch.float64)    \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "    \n",
    "        self.Q_xx[t,:,:] = (self.R_xx[t,:,:].clone() \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )\n",
    "\n",
    "\n",
    "        self.Q_ux[t,:,:] = (self.R_ux[t,:,:].clone() \n",
    "                            + 2 * self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) \n",
    "                           )\n",
    "    \n",
    "        self.Q_uu[t,:,:] = (self.R_uu[t,:,:].clone()  \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() )\n",
    "                            - self.Omega_mat.clone()\n",
    "                           )\n",
    "\n",
    "\n",
    "        self.Q_x[t,:] = self.R_x[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone()) \n",
    "        self.Q_u[t,:] = self.R_u[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone())\n",
    "        self.Q_0[t]   = self.R_0[t].clone() + self.gamma * self.F_0[t+1].clone()\n",
    "\n",
    "\n",
    "        \n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = (self.beta.clone() * self.Q_ux[t,:,:].clone() \n",
    "               + self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone()))\n",
    "        W_t = (self.beta.clone() * self.Q_u[t,:].clone() \n",
    "               +  self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:]).clone())\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        Sigma_p_bar_inv = torch.inverse(Sigma_p_bar + self.reg_mat)\n",
    "        \n",
    "        # update parameters of F-function\n",
    "        self.F_xx[t,:,:] = self.Q_xx[t,:,:].clone() + (1/(2*self.beta.clone()))*(U_t.t().mm(Sigma_p_bar_inv.clone().mm(U_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().t().mm(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_x[t,:] = self.Q_x[t,:].clone() + (1/self.beta.clone())*(U_t.mv(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().mv(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_0[t] = self.Q_0[t].clone() + ( (1/(2*self.beta.clone()))*(W_t.dot(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.u_bar_prior[t,:].clone().dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "                                    - (1/(2*self.beta.clone())) * (torch.log(torch.det(self.Sigma_prior[t,:,:].clone()+\n",
    "                                                                              self.reg_mat))\n",
    "                                                       - torch.log(torch.det(Sigma_p_bar_inv.clone() + self.reg_mat))) )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        \n",
    "        Sigma_prior_new = torch.inverse(new_Sigma_prior_inv + self.reg_mat)\n",
    "        \n",
    "        # update parameters using the previous value of Sigma_prior_inv\n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.mv(self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_u[t,:].clone())\n",
    "        \n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.clone().mm(self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_ux[t,:,:].clone())\n",
    "        \n",
    "        # and then assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new.clone()\n",
    "        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv.clone()\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:].clone()\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:].clone()\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:].clone()\n",
    "            \n",
    "    def trajs_to_torch_tensors(self,trajs):\n",
    "        \"\"\"\n",
    "        Convert data from a list of lists into Torch tensors\n",
    "        \"\"\"\n",
    "        num_trajs = len(trajs)\n",
    "        \n",
    "        self.data_xvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "        self.data_uvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "            \n",
    "        for n in range(num_trajs):\n",
    "            for t in range(self.num_steps):\n",
    "                self.data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64).clone()\n",
    "                self.data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64).clone()\n",
    "                \n",
    "    def compute_reward_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.R_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.R_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.R_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.R_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.R_u[t,:].clone())\n",
    "        aux_0 = self.R_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_G_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.Q_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.Q_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.Q_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.Q_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.Q_u[t,:].clone())\n",
    "        aux_0 = self.Q_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_F_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.F_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_x = x_t.dot(self.F_x[t,:].clone())\n",
    "        aux_0 = self.F_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_x + aux_0\n",
    "\n",
    "                 \n",
    "    def MaxEntIRL(self,\n",
    "                  trajs,\n",
    "                  learning_rate,\n",
    "                  err_tol, max_iter):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate parameters of the reward function using MaxEnt IRL.\n",
    "        Inputs:\n",
    "        \n",
    "        trajs - a list of trajectories. Each trajectory is a list of state-action pairs, stored as a tuple.\n",
    "                We assume each trajectory has the same length\n",
    "        \"\"\"\n",
    "        \n",
    "        # omega is a tunable parameter that determines the cost matrix self.Omega_mat\n",
    "        omega_init = 15.0\n",
    "        self.omega = torch.tensor(omega_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        # also set beta to be small \n",
    "        beta_init = 50 \n",
    "        self.beta = torch.tensor(beta_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        reward_params =  [self.lambd, self.eta, self.rho, self.omega, self.beta]\n",
    "        \n",
    "        print(\"Omega mat...\")\n",
    "        self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        print(\"g learning...\")\n",
    "        self.reset_prior_policy()\n",
    "        self.G_learning(err_tol, max_iter)\n",
    "        print(\"intialize optimizer...\")\n",
    "        optimizer = optim.Adam(reward_params, lr=learning_rate)\n",
    "        print(\"zero grad...\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        num_trajs = len(trajs)\n",
    "        print(\"trajs_to_torch_tensors...\")\n",
    "        # fill in Torch tensors for the trajectory data\n",
    "        self.trajs_to_torch_tensors(trajs)\n",
    "        print(\"constructing zero tensors...\")   \n",
    "        self.realized_rewards = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        self.realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"done...\")  \n",
    "        \n",
    "        num_iter_IRL = 3\n",
    "        \n",
    "        for i in range(num_iter_IRL):\n",
    "            \n",
    "            print('GIRL iteration = ', i)\n",
    "       \n",
    "            self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "    \n",
    "            for n in range(101):\n",
    "                if n%100==0:\n",
    "                    print(n)\n",
    "                for t in range(self.num_steps):\n",
    "                    \n",
    "                    \n",
    "                    # compute rewards obtained at each step for each trajectory\n",
    "                    # given the model paramaters\n",
    "        \n",
    "                    self.realized_rewards[n,t] = self.compute_reward_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                                                                \n",
    "            \n",
    "                    # compute the log-likelihood by looping over trajectories\n",
    "                    self.realized_G_fun[n,t] = self.compute_G_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                \n",
    "                \n",
    "                    self.realized_F_fun[n,t] = self.compute_F_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:])\n",
    "                \n",
    "\n",
    "                self.realized_cum_rewards[n] = self.realized_rewards[n,:].sum().clone()\n",
    "                self.realized_G_fun_cum[n] = self.realized_G_fun[n,:].sum().clone()\n",
    "                self.realized_F_fun_cum[n] = self.realized_F_fun[n,:].sum().clone()\n",
    "            \n",
    "            # the negative log-likelihood will not include terms ~ Sigma_p as we do not optimize over its value\n",
    "            loss = - self.beta.clone()*(self.realized_G_fun_cum.sum().clone() - self.realized_F_fun_cum.sum().clone())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            loss.backward() \n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            print('Iteration = ', i)\n",
    "            print('Loss = ', loss.detach().numpy())\n",
    "        \n",
    "           \n",
    "        print('Done optimizing reward parameters')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the G-learning portfolio optimization class\n",
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 params,\n",
    "                 beta,\n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky assets\n",
    "                 init_x_vals, # array of initial asset position values (num_risky_assets + 1)\n",
    "                 use_for_WM = True): # use for wealth management tasks\n",
    "\n",
    "                \n",
    "        self.num_steps = num_steps\n",
    "        self.num_assets = num_risky_assets + 1 \n",
    "        \n",
    "        self.lambd = torch.tensor(params[0], requires_grad=False, dtype=torch.float64)\n",
    "        self.Omega_mat = params[1] * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        self.eta = torch.tensor(params[2], requires_grad=False, dtype=torch.float64)\n",
    "        self.rho = torch.tensor(params[3], requires_grad=False, dtype=torch.float64)\n",
    "        self.beta = torch.tensor(beta, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_risky_assets = num_risky_assets\n",
    "        self.r_f = riskfree_rate\n",
    "        \n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets\n",
    "        \n",
    "        self.Sigma_r_np = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        self.reg_mat = 1e-3*torch.eye(self.num_assets, dtype=torch.float64)\n",
    "        \n",
    "        # arrays of returns for all assets including the risk-free asset\n",
    "        # array of shape num_steps x (num_stocks + 1) \n",
    "        self.exp_returns_np = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))\n",
    "                                      \n",
    "        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside\n",
    "        self.Sigma_r_tilde_np = np.zeros((self.num_assets, self.num_assets))\n",
    "        self.Sigma_r_tilde_np[1:,1:] = self.Sigma_r_np\n",
    "            \n",
    "        # make Torch tensors  \n",
    "        self.exp_returns = torch.tensor(self.exp_returns_np,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r = torch.tensor(Sigma_r,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r_tilde = torch.tensor(self.Sigma_r_tilde_np,requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.benchmark_portf = torch.tensor(benchmark_portf, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        # asset holding values for all times. Initialize with initial values, \n",
    "        # values for the future times will be expected values \n",
    "        self.x_vals_np = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.x_vals_np[0,:] = init_x_vals \n",
    "        \n",
    "        # Torch tensor\n",
    "        self.x_vals = torch.tensor(self.x_vals_np)\n",
    "                \n",
    "        # allocate memory for coefficients of R-, F- and G-functions        \n",
    "        self.F_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets, dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.F_x = torch.zeros(self.num_steps, self.num_assets, dtype=torch.float64,\n",
    "                               requires_grad=True)\n",
    "        self.F_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.Q_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.R_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "        \n",
    "        self.reset_prior_policy()\n",
    "        \n",
    "        # the list of adjustable model parameters:\n",
    "        self.model_params = [self.lambd, self.beta, self.Omega_mat, self.eta]         \n",
    "        \n",
    "        # expected cash installment for all steps\n",
    "        self.expected_c_t = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # realized values of the target portfolio\n",
    "        self.realized_target_portf = np.zeros(self.num_steps,dtype=np.float64)\n",
    "        \n",
    "        # expected portfolio values for all times\n",
    "        self.expected_portf_val = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # the first value is the sum of initial position values\n",
    "        self.expected_portf_val[0] = self.x_vals[0,:].sum()\n",
    "\n",
    "    def reset_prior_policy(self):\n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = torch.zeros(self.num_steps,self.num_assets,requires_grad=False,\n",
    "                                       dtype=torch.float64)\n",
    "        self.v_bar_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior_inv = torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior_inv[t,:,:] = 10.0 * torch.eye(self.num_assets).clone() # np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "    \n",
    "    def reward_fun(self, t, x_vals, u_vals, exp_rets, lambd, Sigma_hat):\n",
    "        \"\"\"\n",
    "        The reward function \n",
    "        \"\"\"\n",
    "        x_plus = x_vals + u_vals\n",
    "        \n",
    "        p_hat = self.rho.clone() * self.benchmark_portf[t] + (1-self.rho.clone())*self.eta.clone()*x_vals.sum()\n",
    "        \n",
    "        aux_1 = - self.lambd.clone() * p_hat**2         \n",
    "        aux_2 = - u_vals.sum()   \n",
    "        aux_3 = 2*self.lambd.clone() * p_hat * x_plus.dot(torch.ones(num_assets) + exp_rets)\n",
    "        aux_4 = - self.lambd.clone() * x_plus.mm(Sigma_hat.mv(x_plus))\n",
    "        aux_5 = - u_vals.mm(self.Omega_mat.clone().mv(u_vals))\n",
    "        \n",
    "        return aux_1 + aux_2 + aux_3 + aux_4 + aux_5  \n",
    "    \n",
    "    def compute_reward_fun(self):\n",
    "        \"\"\"\n",
    "        Compute coefficients R_xx, R_ux, etc. for all steps\n",
    "        \"\"\"\n",
    "        for t in range(0, self.num_steps):\n",
    "            \n",
    "            one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "            benchmark_portf = self.benchmark_portf[t]\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            with torch.no_grad():\n",
    "                self.R_xx[t,:,:] = (-self.lambd.clone()*(self.eta.clone()**2)*(self.rho.clone()**2)*one_one_T_mat\n",
    "                                     + 2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                     - self.lambd.clone()*Sigma_hat)\n",
    "\n",
    "                self.R_ux[t,:,:] = (2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                     - 2*self.lambd.clone()*Sigma_hat)\n",
    "\n",
    "                self.R_uu[t,:,:] = - self.lambd.clone() * Sigma_hat - self.Omega_mat.clone()\n",
    "\n",
    "                self.R_x[t,:] =  (-2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*(1-self.rho.clone())*benchmark_portf *\n",
    "                                     torch.ones(self.num_assets,dtype=torch.float64)\n",
    "                                     + 2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret)\n",
    "\n",
    "                self.R_u[t,:] = (2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret\n",
    "                                 - torch.ones(self.num_assets,dtype=torch.float64))\n",
    "\n",
    "                self.R_0[t] = - self.lambd.clone()*((1-self.rho.clone())**2) * (benchmark_portf**2)\n",
    "                \n",
    "         \n",
    "    def project_cash_injections(self):\n",
    "        \"\"\"\n",
    "        Compute the expected values of future asset positions, and the expected cash injection for future steps,\n",
    "        as well as realized values of the target portfolio\n",
    "        \"\"\"\n",
    "           \n",
    "        # this assumes that the policy is trained\n",
    "        for t in range(1, self.num_steps):  # the initial value is fixed \n",
    "            # increment the previous x_t\n",
    "            delta_x_t = self.u_bar_prior[t,:] + self.v_bar_prior[t,:,:].mv(self.x_vals[t-1,:])\n",
    "            self.x_vals[t,:] = self.x_vals[t-1,:] + delta_x_t\n",
    "            \n",
    "            # grow using the expected return\n",
    "            self.x_vals[t,:] = (torch.ones(self.num_assets)+ self.exp_returns[t,:])*self.x_vals[t,:]\n",
    "            \n",
    "            # compute c_t\n",
    "            self.expected_c_t[t] = delta_x_t.sum().data # detach().numpy()\n",
    "            \n",
    "            # expected portfolio value for this step\n",
    "            self.expected_portf_val[t] = self.x_vals[t,:].sum().data # .detach().numpy()\n",
    "                                                                                      \n",
    "    def set_terminal_conditions(self):\n",
    "        \"\"\"\n",
    "        set the terminal condition for the F-function\n",
    "        \"\"\"\n",
    "        \n",
    "        # the auxiliary quantity to perform matrix calculations\n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[-1,:]\n",
    "        \n",
    "        # Compute the reward function for all steps (only the last step is needed for this functions, while \n",
    "        # values for other time steps will be used in other functions)\n",
    "        self.compute_reward_fun()\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            Sigma_hat_inv = torch.inverse(Sigma_hat + self.reg_mat)\n",
    "            \n",
    "            Sigma_tilde = Sigma_hat + (1/self.lambd)*self.Omega_mat.clone()\n",
    "            Sigma_tilde_inv = torch.inverse(Sigma_tilde + self.reg_mat)\n",
    "            \n",
    "            Sigma_hat_sigma_tilde = Sigma_hat.mm(Sigma_tilde)\n",
    "            Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.mm(Sigma_hat)\n",
    "            Sigma_tilde_sigma_hat = Sigma_tilde.mm(Sigma_hat)\n",
    "            \n",
    "            Sigma_hat_Sigma_tilde_inv = Sigma_hat.mm(Sigma_tilde_inv)\n",
    "            Sigma_3_plus_omega = self.lambd*Sigma_tilde_inv.mm(Sigma_hat_Sigma_tilde_inv) + self.Omega_mat.clone()    \n",
    "                             \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            Sigma_tilde_inv_t_R_ux = Sigma_tilde_inv.t().mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.t().mm(self.R_uu[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_u = Sigma_tilde_inv.t().mv(self.R_u[-1,:].clone())\n",
    "            \n",
    "            Sigma_tilde_inv_R_u = Sigma_tilde_inv.mv(self.R_u[-1,:].clone())\n",
    "            Sigma_tilde_inv_R_ux = Sigma_tilde_inv.mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.mm(self.R_uu[-1,:,:].clone())\n",
    "            \n",
    "            # though the action at the last step is deterministic, we can feed \n",
    "            # parameters of the prior with these values                     \n",
    "              \n",
    "            self.u_bar_prior[-1,:]   = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mv(self.R_u[-1,:].clone())\n",
    "            self.v_bar_prior[-1,:,:] = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())    \n",
    "                \n",
    "            # First compute the coefficients of the reward function F at the last step:        \n",
    "            # F_xx       \n",
    "            with torch.no_grad():\n",
    "                self.F_xx[-1,:,:] = (self.R_xx[-1,:,:].clone()\n",
    "                                     + (1/(2*self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mm(Sigma_tilde_inv_t_R_ux)\n",
    "                                     + (1/(4*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mm(\n",
    "                                          Sigma_tilde_inv_t_R_uu.clone().mm(Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())))\n",
    "                                    )\n",
    "\n",
    "                # F_x                    \n",
    "                self.F_x[-1,:] = (self.R_x[-1,:].clone()\n",
    "                                     + (1/(self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mv(Sigma_tilde_inv_t_R_u.clone())\n",
    "                                     + (1/(2*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mv(\n",
    "                                          Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                                )\n",
    "\n",
    "                # F_0   \n",
    "                self.F_0[-1] = (self.R_0[-1].clone() \n",
    "                                +  (1/(2*self.lambd.clone()))* self.R_u[-1,:].clone().dot(Sigma_tilde_inv_R_u.clone())\n",
    "                                + (1/(4*self.lambd.clone()**2))* self.R_u[-1,:].clone().dot(\n",
    "                                    Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                               )\n",
    "\n",
    "                # for the Q-function at the last step:\n",
    "                self.Q_xx[-1,:,:] = self.R_xx[-1,:,:].clone()\n",
    "                self.Q_ux[-1,:,:] = self.R_ux[-1,:,:].clone()\n",
    "                self.Q_uu[-1,:,:] = self.R_uu[-1,:,:].clone()\n",
    "                self.Q_u[-1,:] = self.R_u[-1,:].clone()\n",
    "                self.Q_x[-1,:] = self.R_x[-1,:].clone()\n",
    "                self.Q_0[-1] = self.R_0[-1].clone()\n",
    "\n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"   \n",
    "        print('Doing G-learning, it may take a few seconds...')\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # make matrix Sigma_hat_t        \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "        Sigma_hat_t = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "        \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = torch.diag(torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:])\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].clone()  \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].clone()     \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = torch.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*torch.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "            tol = torch.max(err_u_bar, err_v_bar)  # tol = 0.5*(err_u_bar + err_v_bar)\n",
    "            \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "                \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "                \n",
    "        ones = torch.ones(self.num_assets,dtype=torch.float64)    \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "        with torch.no_grad():\n",
    "            self.Q_xx[t,:,:] = (self.R_xx[t,:,:].clone() \n",
    "                                + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                               + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )\n",
    "\n",
    "\n",
    "            self.Q_ux[t,:,:] = (self.R_ux[t,:,:].clone() \n",
    "                                + 2 * self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                               + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) \n",
    "                               )\n",
    "\n",
    "            self.Q_uu[t,:,:] = (self.R_uu[t,:,:].clone()  \n",
    "                                + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                               + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() )\n",
    "                                - self.Omega_mat.clone()\n",
    "                               )\n",
    "\n",
    "\n",
    "            self.Q_x[t,:] = self.R_x[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone()) \n",
    "            self.Q_u[t,:] = self.R_u[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone())\n",
    "            self.Q_0[t]   = self.R_0[t].clone() + self.gamma * self.F_0[t+1].clone()\n",
    "\n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = (self.beta.clone() * self.Q_ux[t,:,:].clone() \n",
    "               + self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone()))\n",
    "        W_t = (self.beta.clone() * self.Q_u[t,:].clone() \n",
    "               +  self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:]).clone())\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        Sigma_p_bar_inv = torch.inverse(Sigma_p_bar + self.reg_mat)\n",
    "        with torch.no_grad():\n",
    "        # update parameters of F-function\n",
    "            self.F_xx[t,:,:] = self.Q_xx[t,:,:].clone() + (1/(2*self.beta.clone()))*(U_t.t().mm(Sigma_p_bar_inv.clone().mm(U_t))\n",
    "                                        - self.v_bar_prior[t,:,:].clone().t().mm(\n",
    "                                            self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())))\n",
    "\n",
    "\n",
    "            self.F_x[t,:] = self.Q_x[t,:].clone() + (1/self.beta.clone())*(U_t.mv(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                        - self.v_bar_prior[t,:,:].clone().mv(\n",
    "                                            self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "\n",
    "\n",
    "            self.F_0[t] = self.Q_0[t].clone() + ( (1/(2*self.beta.clone()))*(W_t.dot(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                        - self.u_bar_prior[t,:].clone().dot(\n",
    "                                            self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "                                        - (1/(2*self.beta.clone())) * (torch.log(torch.det(self.Sigma_prior[t,:,:].clone()+\n",
    "                                                                                  self.reg_mat))\n",
    "                                                           - torch.log(torch.det(Sigma_p_bar_inv.clone() + self.reg_mat))) )\n",
    "\n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "\n",
    "        Sigma_prior_new = torch.inverse(new_Sigma_prior_inv + self.reg_mat)\n",
    "        \n",
    "        # update parameters using the previous value of Sigma_prior_inv\n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.mv(self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_u[t,:].clone())\n",
    "        \n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.clone().mm(self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_ux[t,:,:].clone())\n",
    "        \n",
    "        # and then assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new.clone()\n",
    "        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv.clone()\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:].clone()\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:].clone()\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:].clone()\n",
    "            \n",
    "    def trajs_to_torch_tensors(self,trajs):\n",
    "        \"\"\"\n",
    "        Convert data from a list of lists into Torch tensors\n",
    "        \"\"\"\n",
    "        num_trajs = len(trajs)\n",
    "        \n",
    "        self.data_xvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "        self.data_uvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "            \n",
    "        for n in range(num_trajs):\n",
    "            for t in range(self.num_steps):\n",
    "                self.data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64).clone()\n",
    "                self.data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64).clone()\n",
    "                \n",
    "    def compute_reward_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.R_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.R_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.R_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.R_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.R_u[t,:].clone())\n",
    "        aux_0 = self.R_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_G_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.Q_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.Q_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.Q_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.Q_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.Q_u[t,:].clone())\n",
    "        aux_0 = self.Q_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_F_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.F_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_x = x_t.dot(self.F_x[t,:].clone())\n",
    "        aux_0 = self.F_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_x + aux_0\n",
    "                 \n",
    "    def MaxEntIRL(self,\n",
    "                  trajs,\n",
    "                  learning_rate,\n",
    "                  err_tol, max_iter):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate parameters of the reward function using MaxEnt IRL.\n",
    "        Inputs:\n",
    "        \n",
    "        trajs - a list of trajectories. Each trajectory is a list of state-action pairs, stored as a tuple.\n",
    "                We assume each trajectory has the same length\n",
    "        \"\"\"\n",
    "        \n",
    "        # omega is a tunable parameter that determines the cost matrix self.Omega_mat\n",
    "        omega_init = 15.0\n",
    "        self.omega = torch.tensor(omega_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        beta_init = 50 # Beta is fixed and not a learned parameter.\n",
    "        self.beta = torch.tensor(beta_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        reward_params =  [self.lambd, self.eta, self.rho, self.omega, self.beta]\n",
    "        \n",
    "        print(\"Omega mat...\")\n",
    "        self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        print(\"g learning...\")\n",
    "        self.reset_prior_policy()\n",
    "        self.G_learning(err_tol, max_iter)\n",
    "        print(\"intialize optimizer...\")\n",
    "        optimizer = optim.Adam(reward_params, lr=learning_rate)\n",
    "        print(\"zero grad...\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        num_trajs = len(trajs)\n",
    "        print(\"trajs_to_torch_tensors...\")\n",
    "        \n",
    "        # fill in Torch tensors for the trajectory data\n",
    "        self.trajs_to_torch_tensors(trajs)\n",
    "        print(\"constructing zero tensors...\")   \n",
    "        self.realized_rewards = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        self.realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"done...\")  \n",
    "        \n",
    "        num_iter_IRL = 3\n",
    "        \n",
    "        for i in range(num_iter_IRL):\n",
    "            print('GIRL iteration = ', i)\n",
    "    \n",
    "            self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "    \n",
    "            for n in range(101):\n",
    "                if n%100==0:\n",
    "                    print(n)\n",
    "                for t in range(self.num_steps):\n",
    "                    \n",
    "                    # compute rewards obtained at each step for each trajectory\n",
    "                    # given the model parameters\n",
    "                    self.realized_rewards[n,t] = self.compute_reward_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                                                                \n",
    "            \n",
    "                    # compute the log-likelihood by looping over trajectories\n",
    "                    self.realized_G_fun[n,t] = self.compute_G_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                \n",
    "                \n",
    "                    self.realized_F_fun[n,t] = self.compute_F_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:])\n",
    "                \n",
    "\n",
    "                self.realized_cum_rewards[n] = self.realized_rewards[n,:].sum().clone()\n",
    "                self.realized_G_fun_cum[n] = self.realized_G_fun[n,:].sum().clone()\n",
    "                self.realized_F_fun_cum[n] = self.realized_F_fun[n,:].sum().clone()\n",
    "            \n",
    "            # the negative log-likelihood will not include terms ~ Sigma_p as we do not optimize over its value\n",
    "            loss = - self.beta.clone()*(self.realized_G_fun_cum.sum().clone() - self.realized_F_fun_cum.sum().clone())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            loss.backward() \n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            print('Iteration = ', i)\n",
    "            print('Loss = ', loss.detach().numpy())\n",
    "        \n",
    "           \n",
    "        print('Done optimizing reward parameters')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_8XeDlpPQOV"
   },
   "source": [
    "## Simulate portfolio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gy2u0fkyPQOX"
   },
   "source": [
    "### Simulate the market factor under a lognormal distribution with a fixed drift and vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F25fZ-s9PQOb"
   },
   "outputs": [],
   "source": [
    "mu_market = 0.05\n",
    "vol_market = 0.25\n",
    "init_market_val = 100.0\n",
    "\n",
    "r_rf = 0.02  # risk-free rate - the first asset will be cash\n",
    "\n",
    "num_steps = 10\n",
    "dt = 0.25 # quarterly time steps\n",
    "\n",
    "num_risky_assets = 99 \n",
    "\n",
    "returns_market = np.zeros(num_steps)\n",
    "market_vals = np.zeros(num_steps)\n",
    "market_vals[0] = 100.0  # initial value\n",
    "\n",
    "\n",
    "        \n",
    "for t in range(1,num_steps):\n",
    "\n",
    "        rand_norm = np.random.randn()\n",
    "        \n",
    "        # use log-returns of market as 'returns_market'\n",
    "        returns_market[t] = mu_market * dt + vol_market * np.sqrt(dt) * rand_norm\n",
    "        \n",
    "        market_vals[t] = market_vals[t-1] * np.exp((mu_market - 0.5*vol_market**2)*dt + \n",
    "                                                         vol_market*np.sqrt(dt)*rand_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUUBRbwbPQO1"
   },
   "source": [
    "### Simulate market betas and idiosyncratic alphas within pre-defined ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "qraR7fRVPQO4",
    "outputId": "dfb260b8-fa17-4a70-a12d-86a4c2964e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34611122 0.34355205 0.81244321 0.53273875 0.23397714 0.26997951\n",
      " 0.32674885 0.35816092 0.70250379 0.53801522]\n",
      "[-0.04383049 -0.0117318   0.03629404  0.00155883 -0.00775654  0.04270811\n",
      " -0.01588382 -0.04914357  0.0979936   0.11883517]\n"
     ]
    }
   ],
   "source": [
    "beta_min = 0.05\n",
    "beta_max = 0.85\n",
    "beta_vals = np.random.uniform(low=beta_min, high=beta_max, size=num_risky_assets)\n",
    "\n",
    "alpha_min = - 0.05\n",
    "alpha_max = 0.15\n",
    "alpha_vals = np.random.uniform(low=alpha_min, high=alpha_max, size=num_risky_assets)\n",
    "\n",
    "print(beta_vals[0:10])\n",
    "print(alpha_vals[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNFzG5dgPQPB"
   },
   "source": [
    "### Simulate time-dependent expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a36GRMAlPQPD"
   },
   "outputs": [],
   "source": [
    "# Time-independent expected returns would be equal to alpha + beta * expected_market_return \n",
    "# Make them time-dependent (and correlated with actual returns) as alpha + beta * oracle_market_returns\n",
    "\n",
    "oracle_coeff = 0.2\n",
    "mu_vec = mu_market * np.ones(num_steps)\n",
    "oracle_market_returns = mu_vec * dt + oracle_coeff*(returns_market - mu_vec)\n",
    "\n",
    "expected_risky_returns = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "for t in range(num_steps):\n",
    "    expected_risky_returns[t,:] = alpha_vals * dt + beta_vals * oracle_market_returns[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDN_R3znPQPK"
   },
   "source": [
    "### Initial values of all assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YneTj6IBPQPM"
   },
   "outputs": [],
   "source": [
    "val_min = 20.0\n",
    "val_max = 120.0\n",
    "\n",
    "init_risky_asset_vals = np.random.uniform(low=val_min, high=val_max, size=num_risky_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqd7WIxbPQPS"
   },
   "source": [
    "### Simulate realized returns and asset prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSL2TU1IPQPW"
   },
   "outputs": [],
   "source": [
    "# Generate realized returns and realized asset values by simulating from a one-factor model \n",
    "# with time-dependent expected returns\n",
    "\n",
    "risky_asset_returns = np.zeros((num_steps, num_risky_assets))\n",
    "risky_asset_vals = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "idiosync_vol =  0.05 # vol_market \n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "    # asset returns are simulated from a one-factor model\n",
    "    risky_asset_returns[t,:] = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "    # asset values\n",
    "    if t == 0:\n",
    "        risky_asset_vals[t,:] = init_risky_asset_vals\n",
    "    else:\n",
    "        risky_asset_vals[t] = risky_asset_vals[t-1] * (1 + risky_asset_returns[t,:])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "YvHvV7B6PQPh",
    "outputId": "3421ed28-a65f-4362-d166-990d22dbf1fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the realized and expected returns\n",
    "# Note that they should appear correlated.\n",
    "\n",
    "# pick a random asset ID to show\n",
    "asset_idx =  4 \n",
    "\n",
    "plt.plot(expected_risky_returns[:,asset_idx],label='expected_return')\n",
    "plt.plot(risky_asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Realized returns vs expected returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqAdjcoFPQPp"
   },
   "source": [
    "### Compute the empirical correlation matrix using realized returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sl5JQ3_8PQPs",
    "outputId": "5f93d784-2e83-4a2c-bf2e-23bd42eb5fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99)\n"
     ]
    }
   ],
   "source": [
    "cov_mat_r = np.cov(risky_asset_returns.T) \n",
    "\n",
    "print(cov_mat_r.shape)\n",
    "\n",
    "D,v = np.linalg.eigh(cov_mat_r)\n",
    "\n",
    "eigenvals = D[::-1]  # put them in a descended order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "H-HzoeHOPQP0",
    "outputId": "96c11580-037d-430d-fe77-5e54635ea519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.24505344e-01, 8.18076755e-03, 7.73408065e-03, 6.18855100e-03,\n",
       "       5.21281280e-03, 5.11413639e-03, 4.70449591e-03, 3.90889376e-03,\n",
       "       3.03591277e-03, 4.94785706e-17])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues: the largest eigenvalue is the market factor \n",
    "eigenvals[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "hFC61Ci94nnd",
    "outputId": "fc1d8cde-9ada-4e39-f6e6-3fe3b55559e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 31.4911, -11.9142,   1.8406,  ...,   2.2557,  11.5814,  19.8107],\n",
       "        [-11.9142,  16.3379,  -5.3456,  ...,  -2.2719,  -9.6478,  -4.0091],\n",
       "        [  1.8406,  -5.3456,   5.2019,  ...,   0.3933,   2.1584,   0.1599],\n",
       "        ...,\n",
       "        [  2.2557,  -2.2719,   0.3933,  ...,   3.8230,  -4.2528,  -6.6905],\n",
       "        [ 11.5814,  -9.6478,   2.1584,  ...,  -4.2528,  28.2281,  12.8106],\n",
       "        [ 19.8107,  -4.0091,   0.1599,  ...,  -6.6905,  12.8106,  36.8192]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_torch = torch.tensor(cov_mat_r)\n",
    "\n",
    "torch.pinverse(cov_mat_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohSC6oDePQP5"
   },
   "source": [
    "### Add a riskless bond as one more asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7nwaM28PQP9"
   },
   "outputs": [],
   "source": [
    "num_assets = num_risky_assets + 1\n",
    "\n",
    "bond_val = 100.0\n",
    "\n",
    "# add the bond to initial assets\n",
    "init_asset_vals = np.hstack((np.array([bond_val]),\n",
    "                            init_risky_asset_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZQgn_CTPQQE"
   },
   "source": [
    "### Make the initial portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyMLBotBPQQH"
   },
   "outputs": [],
   "source": [
    "# consider here two choices: equal or equally-weighted \n",
    "\n",
    "init_port_choice =  'equal' \n",
    "\n",
    "init_cash = 1000.0\n",
    "init_total_asset = np.sum(init_asset_vals)\n",
    "\n",
    "x_vals_init = np.zeros(num_assets)\n",
    "\n",
    "if init_port_choice == 'equal': \n",
    "    # hold equal amounts of cash in each asset\n",
    "    amount_per_asset = init_cash/num_assets\n",
    "    x_vals_init = amount_per_asset * np.ones(num_assets)\n",
    "\n",
    "elif init_port_choice == 'equally_weighted':\n",
    "    amount_per_asset = init_cash/init_total_asset\n",
    "    x_vals_init = amount_per_asset * init_asset_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zkj8GdPDPQQP"
   },
   "source": [
    "### Make the target portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LyyKCaPGPQQS",
    "outputId": "bda0d8d5-b848-4c2b-c36b-3f2ac1a1bf78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100.0 1541.5835692311712\n"
     ]
    }
   ],
   "source": [
    "# make a target portfolio term structure by defining it as the initial portfolio growing at some fixed and high rate\n",
    "\n",
    "target_portfolio = [init_cash]\n",
    "\n",
    "target_return = 0.15 \n",
    "coeff_target = 1.1 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    target_portfolio.append(target_portfolio[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "target_portfolio = coeff_target*np.array(target_portfolio)    \n",
    "print(target_portfolio[0], target_portfolio[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UMA8UwlPQQc"
   },
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5G_Y87m2PQQe",
    "outputId": "0cf27618-1f16-4676-9741-4e2c3ce8d466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133.1484530668263 3490.3429574618413\n"
     ]
    }
   ],
   "source": [
    "riskfree_rate = 0.02\n",
    "fee_bond = 0.05 \n",
    "fee_stock = 0.05 \n",
    "\n",
    "all_fees = np.zeros(num_risky_assets + 1)\n",
    "all_fees[0] = fee_bond\n",
    "all_fees[1:] = fee_stock\n",
    "Omega_mat = np.diag(all_fees)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "lambd = 0.001 \n",
    "Omega_mat = 15.5 * np.diag(all_fees) \n",
    "eta = 1.5 \n",
    "\n",
    "beta = 100.0\n",
    "gamma = 0.95 \n",
    "\n",
    "exp_returns = expected_risky_returns\n",
    "\n",
    "Sigma_r = cov_mat_r\n",
    "\n",
    "# Generate the benchmark target portfolio by growing the initial portfolio value at rate eta\n",
    "\n",
    "target_return =  0.5 \n",
    "benchmark_portf = [ init_cash   * np.exp(dt * target_return)]\n",
    "\n",
    "rho = 0.4 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    benchmark_portf.append(benchmark_portf[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "print(benchmark_portf[0], benchmark_portf[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YT6003xKPQQm"
   },
   "source": [
    "### Simulate portfolio data\n",
    "\n",
    "Produce a list of trajectories, where each trajectory is a list made of state-action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "So89W92zPLMq"
   },
   "outputs": [],
   "source": [
    "lambd = 0.001 \n",
    "omega = 1.0 \n",
    "beta = 1000.0 # fixed\n",
    "eta = 1.5 \n",
    "rho = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCzgn95o8wUD"
   },
   "outputs": [],
   "source": [
    "reward_params=[lambd, omega, eta, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58NX65KOPQRD"
   },
   "outputs": [],
   "source": [
    "# Create a G-learner\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 reward_params,  \n",
    "                 beta,                \n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 expected_risky_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky matrix                    \n",
    "                 x_vals_init, # array of initial values of len (num_stocks+1)\n",
    "                 use_for_WM = True) # use for wealth management tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mhy6IPKs7nRq",
    "outputId": "78fbc7d6-2869-48a9-f754-c9ed7c5127c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n"
     ]
    }
   ],
   "source": [
    "G_learner.reset_prior_policy()\n",
    "error_tol=1.e-8 \n",
    "max_iter_RL = 200\n",
    "G_learner.G_learning(error_tol, max_iter_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZJbI_k9mPQQo",
    "outputId": "45fc8ba1-fb6d-450b-8502-b96ffa56419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done simulating trajectories in 16.731534 sec\n"
     ]
    }
   ],
   "source": [
    "num_sim = 1000\n",
    "trajs = []\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "t_0 = time.time()\n",
    "\n",
    "\n",
    "x_vals = [x_vals_init]\n",
    "returns_all = []\n",
    "for n in range(num_sim):\n",
    "    this_traj = []\n",
    "    x_t = x_vals_init[:]\n",
    "    returns_array = []\n",
    "    for t in range(0,num_steps):\n",
    "        \n",
    "        \n",
    "       \n",
    "        mu_t = G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:].mv(torch.tensor(x_t))\n",
    "        u_t = np.random.multivariate_normal(mu_t.detach().numpy(), G_learner.Sigma_prior[t,:].detach().numpy())\n",
    "        # compute new values of x_t\n",
    "\n",
    "        x_next = x_t +u_t\n",
    "        # grow this with random return\n",
    "        \n",
    "        idiosync_vol =  0.05 # vol_market     \n",
    "        rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "        # asset returns are simulated from a one-factor model\n",
    "        risky_asset_returns = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "        returns = np.hstack((riskfree_rate*dt, risky_asset_returns))\n",
    "        \n",
    "        x_next = (1+returns)*x_next\n",
    "        port_returns=(x_next.sum() -x_t.sum() -np.sum(u_t) - 0.015*np.abs(u_t).sum())/x_t.sum()\n",
    "        \n",
    "        this_traj.append((x_t, u_t))\n",
    "        \n",
    "        # rename\n",
    "        x_t = x_next\n",
    "        returns_array.append(port_returns) \n",
    "    # end the loop over time steps\n",
    "    trajs.append(this_traj)\n",
    "    returns_all.append(returns_array)\n",
    "\n",
    "print('Done simulating trajectories in %f sec'% (time.time() - t_0))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpnem5H5vZ5H"
   },
   "source": [
    "### Calculate performance of G-learner (Diagnostics only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z731JGirsdV"
   },
   "outputs": [],
   "source": [
    "returns_all_G=returns_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIYUAIWvO04X",
    "outputId": "7e9f5055-b670-4a22-c6cf-d8082c03c9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13120802121291741\n"
     ]
    }
   ],
   "source": [
    "SR_G=0\n",
    "for i in range(num_sim):\n",
    "    SR_G+=(np.mean(returns_all_G[i])-riskfree_rate*dt)/np.std(returns_all_G[i])\n",
    "\n",
    "SR_G/=num_sim\n",
    "print(SR_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TT9dIJP_t0K_"
   },
   "outputs": [],
   "source": [
    "r_G=np.array([0]*num_steps, dtype='float64')\n",
    "for n in range(num_steps):\n",
    "    for i in range(num_sim):\n",
    "        r_G[n]+=returns_all_G[i][n]\n",
    "    r_G[n]/=num_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "KAdjSXoHui7R",
    "outputId": "9f5ea1f7-38b7-4aff-ef5a-e9ef9788651d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sample Mean Returns')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r_G, label='G-learning (' + str(np.round(SR_G,3)) + ')', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time (quarters)')\n",
    "plt.ylabel('Sample Mean Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTWkeFsDPQQu"
   },
   "outputs": [],
   "source": [
    "np.save('State_act_trajs.npy', trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "SNtCvuFMPQQ0",
    "outputId": "2f12bdef-6454-4811-9d4b-5ee128b9f596"
   },
   "outputs": [],
   "source": [
    "#trajs = np.load('State_act_trajs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Wdu1t058d7a"
   },
   "source": [
    "## GIRL\n",
    "Implement the optimizer for finding the optimal G-learning parameters by MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gH6Ho4m9yHvX"
   },
   "outputs": [],
   "source": [
    "def get_loss(trajs,\n",
    "             num_steps, \n",
    "             benchmark_portf, \n",
    "             gamma, \n",
    "             num_risky_assets, \n",
    "             riskfree_rate, \n",
    "             expected_risky_returns, \n",
    "             Sigma_r, \n",
    "             x_vals_init, \n",
    "             max_iter_RL, \n",
    "             reward_params,\n",
    "             beta, \n",
    "             num_trajs, \n",
    "             grad=False, \n",
    "             eps=1e-7):\n",
    "\n",
    "\n",
    "  data_xvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "  data_uvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "        \n",
    "  for n in range(num_trajs):\n",
    "        for t in range(num_steps):\n",
    "            data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64)\n",
    "            data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64)\n",
    "                \n",
    "                \n",
    "  # allocate memory for tensors that wil be used to compute the forward pass\n",
    "  realized_rewards = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "  realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "  realized_G_fun = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "  realized_F_fun  = torch.zeros(num_trajs,  num_steps, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "  realized_G_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "  realized_F_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "\n",
    "  reward_params_dict={}\n",
    "  loss_dict={}\n",
    "  loss_dict[-1]=np.array([0]*len(reward_params), dtype='float64') # perturb up\n",
    "  loss_dict[1]=np.array([0]*len(reward_params), dtype='float64') # perturb down\n",
    "  loss_grad = np.array([0]*len(reward_params), dtype='float64') \n",
    "\n",
    "  if grad: # compute gradient\n",
    "    for j in range(len(reward_params)):\n",
    "      for k in [-1,1]:\n",
    "            reward_params_dict[k]=reward_params\n",
    "            reward_params_dict[k][j]= reward_params_dict[k][j] + k*eps\n",
    "              \n",
    "            # 1. create a G-learner\n",
    "            G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                             reward_params_dict[k],\n",
    "                                             beta,\n",
    "                                             benchmark_portf,\n",
    "                                             gamma,\n",
    "                                             num_risky_assets,\n",
    "                                             riskfree_rate,\n",
    "                                             expected_risky_returns,\n",
    "                                             Sigma_r,\n",
    "                                             x_vals_init,\n",
    "                                             use_for_WM=True)\n",
    "        \n",
    "            G_learner.reset_prior_policy()\n",
    "        \n",
    "            # run the G-learning recursion to get parameters of G- and F-functions\n",
    "            G_learner.G_learning(error_tol, max_iter_RL)\n",
    "        \n",
    "            # compute the rewards and realized values of G- and F-functions from \n",
    "            # all trajectories\n",
    "            for n in range(num_trajs):\n",
    "              for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                \n",
    "              realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "              realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "              realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "          \n",
    "\n",
    "\n",
    "            \n",
    "            loss_dict[k][j] = - beta *(realized_G_fun_cum.sum() - realized_F_fun_cum.sum())\n",
    "      loss_grad[j]=(loss_dict[1][j]-loss_dict[-1][j])/(2.0*eps)\n",
    "\n",
    "  G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                      reward_params,\n",
    "                                      beta,\n",
    "                                      benchmark_portf,\n",
    "                                      gamma,\n",
    "                                      num_risky_assets,\n",
    "                                      riskfree_rate,\n",
    "                                      expected_risky_returns,\n",
    "                                      Sigma_r,\n",
    "                                      x_vals_init,\n",
    "                                      use_for_WM=True)\n",
    "        \n",
    "  G_learner.reset_prior_policy()\n",
    "        \n",
    "  G_learner.G_learning(error_tol, max_iter_RL)\n",
    "        \n",
    "  # compute the rewards and realized values of G- and F-functions from \n",
    "  # all trajectories\n",
    "  for n in range(num_trajs):\n",
    "      for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                \n",
    "      realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "      realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "      realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "          \n",
    "    \n",
    "\n",
    "  loss = - beta *(realized_G_fun_cum.sum() - realized_F_fun_cum.sum())   \n",
    "  if grad:\n",
    "    return loss, loss_grad\n",
    "  else:\n",
    "    return loss   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2cYmHjoTZKM"
   },
   "outputs": [],
   "source": [
    "def fun(x, grad_=False, rescale=1.0, constraint=False):\n",
    "    y=x.copy()\n",
    "    y[0]/=sc[0]\n",
    "    y[1]/=sc[1]\n",
    "    y[2]/=sc[2]\n",
    "    y[3]/=sc[3]\n",
    "    with torch.no_grad():\n",
    "       if grad_==False:\n",
    "           if constraint:\n",
    "            y[0] = y[0]**2\n",
    "            y[1] = 1+y[1]**2\n",
    "            y[2] = 1+y[2]**2\n",
    "            y[3] = 1.0/(1.0+np.exp(-y[3]))\n",
    "\n",
    "           ret = rescale*get_loss(trajs,\n",
    "                                 num_steps, \n",
    "                                 benchmark_portf, \n",
    "                                 gamma, \n",
    "                                 num_risky_assets, \n",
    "                                 riskfree_rate, \n",
    "                                 expected_risky_returns, \n",
    "                                 Sigma_r, \n",
    "                                 x_vals_init, \n",
    "                                 max_iter_RL, \n",
    "                                 y, \n",
    "                                 beta,\n",
    "                                 len(trajs), \n",
    "                                 grad=grad_, \n",
    "                                 eps=1e-7).detach().numpy()\n",
    "\n",
    "           print(ret)\n",
    "           return ret\n",
    "       else:\n",
    "            f, df = get_loss(trajs,\n",
    "                                 num_steps, \n",
    "                                 benchmark_portf, \n",
    "                                 gamma, \n",
    "                                 num_risky_assets, \n",
    "                                 riskfree_rate, \n",
    "                                 expected_risky_returns, \n",
    "                                 Sigma_r, \n",
    "                                 x_vals_init, \n",
    "                                 max_iter_RL, \n",
    "                                 y, \n",
    "                                 beta,\n",
    "                                 len(trajs), \n",
    "                                 grad=grad_, \n",
    "                                 eps=1e-7)\n",
    "            return f.detach().numpy()*rescale, df*rescale/sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQXvu1HqPQRc"
   },
   "source": [
    "### GIRL (G-learning IRL)\n",
    "Two different optimizers are used to demonstrate the implementation of GIRL. The first approach is gradient free and the second approach uses the gradient of the loss function. \n",
    "#### Gradient free\n",
    "Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58IbcwyOTm9S"
   },
   "outputs": [],
   "source": [
    "lambd_0 = 0.002\n",
    "omega_0 = 1.1\n",
    "eta_0 = 1.3 \n",
    "beta_0 = beta \n",
    "rho_0 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK1h_-r1UCN8"
   },
   "outputs": [],
   "source": [
    "sc=np.array([1,1,1,1]) # optional re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3A81-tocRuz"
   },
   "outputs": [],
   "source": [
    "x0=np.array([lambd_0, omega_0, eta_0, rho_0])\n",
    "x0=[np.sqrt(x0[0]), np.sqrt(x0[1]-1), np.sqrt(x0[2]-1), np.log(x0[3]/(1-x0[3]))]\n",
    "x0*=sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "XdgT7EKxZVDf",
    "outputId": "77a4f480-7d1b-4c78-f021-75f36e51e47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "0.12490919057988376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12490919057988376"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate the loss function\n",
    "fun(x0, False, 1e-9, constraint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "NR9zW91ndDVR",
    "outputId": "aef06036-ee0e-4ab4-cc7f-4ea85627a0a5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "0.12490919057988376\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.2459735668321252\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.11874670620994271\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.14128689142016324\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.12471654349308461\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.047547512969926\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.005363148840919137\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.03679003520976752\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.01679210820861906\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0007476517753824592\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.016222967458084228\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.01282092844489962\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.017729638209156694\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0015885935644954444\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.026365117422245444\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0017697699733749033\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.03237284129664302\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0007972841279059649\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.005953672410875559\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0007049213100224734\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.004316950522862375\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0004546936697587371\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.003917830254860223\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0006796422930210829\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.005435145872652531\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0009310005068331957\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000931\n",
      "         Iterations: 13\n",
      "         Function evaluations: 26\n"
     ]
    }
   ],
   "source": [
    "# Optimize with the Nelder-Mead method\n",
    "res = minimize(fun, x0, method='Nelder-Mead', args=(False, 1e-9, True), options={'disp': True, 'maxiter':50}, tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "HRAO1V1_dtfd",
    "outputId": "2059a172-03ae-499d-e929-0e158baa8afd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[3.88883391e-02, 3.39830169e-01, 5.56786097e-01, 2.69451344e-04],\n",
       "       [3.96902066e-02, 3.47356437e-01, 5.50290007e-01, 1.17187500e-04],\n",
       "       [3.87513660e-02, 3.40989445e-01, 5.61675126e-01, 2.59553030e-04],\n",
       "       [3.95369836e-02, 3.39403071e-01, 5.56639171e-01, 2.64323831e-04],\n",
       "       [3.96670735e-02, 3.34824592e-01, 5.62564058e-01, 2.21180916e-04]]), array([-0.000931  , -0.00074765, -0.00067964, -0.00045469,  0.00070492]))\n",
       "           fun: -0.0009310005068331957\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 26\n",
       "           nit: 13\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([3.88883391e-02, 3.39830169e-01, 5.56786097e-01, 2.69451344e-04])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw2lEM2Tn9tz"
   },
   "outputs": [],
   "source": [
    "# Print parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "D7zhJ8K5ePvA",
    "outputId": "c3025c91-c657-4f33-ed2a-665045ecc375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001512302919276534,\n",
       " 1.1154845438964094,\n",
       " 1.3100107576986522,\n",
       " 0.5000673628356889)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x[0]**2/sc[0], (1+res.x[1]**2)/sc[1], (1+res.x[2]**2)/sc[2], 1.0/(1+np.exp(-res.x[3]))/sc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['$\\lambda$','$\\omega$','$\\eta$','$p$']\n",
    "GT = [0.002,1.1,1.3,0.5]\n",
    "GIRL = [0.001512302919276534,1.1154845438964094,1.3100107576986522,0.5000673628356889]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "rects1 = ax.bar(x - width/2, GT, width, label='Groundtruth')\n",
    "rects2 = ax.bar(x + width/2, GIRL, width, label='GIRL')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Parameter Value')\n",
    "ax.set_title('GIRL Inferred Parameters')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TpYxXmq89QD"
   },
   "source": [
    "#### GIRL (Gradient based)\n",
    "Now separately perform GIRL using a gradient based optimizer. This has the advantage of being more accurate but can be less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPZgTZ789R_H"
   },
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "lambd_0 = 0.002\n",
    "omega_0 = 1.1\n",
    "eta_0 = 1.3 \n",
    "beta_0 = beta \n",
    "rho_0 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9nO_7hHBGai"
   },
   "outputs": [],
   "source": [
    "x0=np.array([lambd_0, omega_0, eta_0, rho_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLHLm-uU9XSu"
   },
   "outputs": [],
   "source": [
    "# rescaling\n",
    "sc=np.array([1000,0.1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "yEO1I-P2DpLp",
    "outputId": "17c8508d-1071-40fe-9735-2a6d37ae705c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "0.9006959304511547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9006959304511547"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluation of the loss function\n",
    "x_ask=np.array([lambd, omega, eta, rho])*sc\n",
    "fun(x_ask, False, 1e-6, constraint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCZdikslULik"
   },
   "outputs": [],
   "source": [
    "# choose bounds for parameters\n",
    "bnds=((0.0001*sc[0], 0.0025*sc[0]), (0.01*sc[1], 1.5*sc[1]), (1.001*sc[2],2*sc[2]), (0.01*sc[3],1*sc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yScEwb3xUcSc",
    "outputId": "597e90db-1c56-4f6a-81d5-3ea9e9a26d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "2220.5208703871153\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2220.5208355104037\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2220.5207604093907\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2220.520864432789\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2220.5208651119383\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.2348110889593124e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "5.9573768129050124e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.2303874114383033e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.862945504447265e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "6.424995689146243e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1039.509103145238\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1039.5094924705213\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1039.5092108942904\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1039.5095057563558\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1039.5094226541369\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2030.3456033245614\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2030.3455363921234\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2030.3457144422036\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2030.3455937407296\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2030.3455935664474\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1087.7432746971622\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1087.7432003031035\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1087.743385211911\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1087.7432274258733\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1087.7431912436225\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3985980685018\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3985129052625\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.398707262162\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3985314391106\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3984693070315\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.1910883029215\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.1914713992252\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.1911967198326\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.1914867322743\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.1914122544229\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3985980685018\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3985129052625\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.398707262162\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3985314391106\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3984693070315\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.2348110889593124e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "5.9573768129050124e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.2303874114383033e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.862945504447265e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "6.424995689146243e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2800.190896035492\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2800.1906895656507\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2800.1909104435217\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2800.190655656457\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2800.1903174798936\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.2874298699721\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.2873390421643\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.2875382821932\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.2873543591274\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1023.2872798157035\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.5099873889797\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.5099011766983\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.5100964487456\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.5099190750383\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.5103175788782\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3480589565373\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3479734992719\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3481681130975\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3479918528236\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1014.3483903415129\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1007.8891632518507\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1007.8890776294469\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1007.8892722984515\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1007.8890956471562\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1007.8894933428614\n",
      "Doing G-learning, it may take a few seconds...\n",
      "983.0573903914168\n",
      "Doing G-learning, it may take a few seconds...\n",
      "983.0573040748947\n",
      "Doing G-learning, it may take a few seconds...\n",
      "983.0574989741742\n",
      "Doing G-learning, it may take a few seconds...\n",
      "983.0573207539916\n",
      "Doing G-learning, it may take a few seconds...\n",
      "983.057715366926\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0941978346333\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0945745584816\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0943040451072\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0945859220884\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.094503359884\n",
      "Doing G-learning, it may take a few seconds...\n",
      "967.6676365003957\n",
      "Doing G-learning, it may take a few seconds...\n",
      "967.6680118351616\n",
      "Doing G-learning, it may take a few seconds...\n",
      "967.6677447505034\n",
      "Doing G-learning, it may take a few seconds...\n",
      "967.6680276369303\n",
      "Doing G-learning, it may take a few seconds...\n",
      "967.6679581674113\n",
      "Doing G-learning, it may take a few seconds...\n",
      "913.1832259490936\n",
      "Doing G-learning, it may take a few seconds...\n",
      "913.1831369061396\n",
      "Doing G-learning, it may take a few seconds...\n",
      "913.1833326253817\n",
      "Doing G-learning, it may take a few seconds...\n",
      "913.1831491801627\n",
      "Doing G-learning, it may take a few seconds...\n",
      "913.1835347012877\n",
      "Doing G-learning, it may take a few seconds...\n",
      "905.3048905502222\n",
      "Doing G-learning, it may take a few seconds...\n",
      "905.3048010844811\n",
      "Doing G-learning, it may take a few seconds...\n",
      "905.3049969325177\n",
      "Doing G-learning, it may take a few seconds...\n",
      "905.3048127741031\n",
      "Doing G-learning, it may take a few seconds...\n",
      "905.3051972227693\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0941978346333\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0945745584816\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0943040451072\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.0945859220884\n",
      "Doing G-learning, it may take a few seconds...\n",
      "901.094503359884\n",
      "Doing G-learning, it may take a few seconds...\n",
      "794.0018227171375\n",
      "Doing G-learning, it may take a few seconds...\n",
      "794.001739471294\n",
      "Doing G-learning, it may take a few seconds...\n",
      "794.0019299588091\n",
      "Doing G-learning, it may take a few seconds...\n",
      "794.0017558482065\n",
      "Doing G-learning, it may take a few seconds...\n",
      "794.0021436531395\n",
      "Doing G-learning, it may take a few seconds...\n",
      "630.3010199015886\n",
      "Doing G-learning, it may take a few seconds...\n",
      "630.3009390883483\n",
      "Doing G-learning, it may take a few seconds...\n",
      "630.3011248087063\n",
      "Doing G-learning, it may take a few seconds...\n",
      "630.3009540517367\n",
      "Doing G-learning, it may take a few seconds...\n",
      "630.3008893661871\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51197963381559\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51191981087625\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.5120348809883\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51192592490837\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51185757639259\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.0937487526205748e+16\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.551952569225996e+16\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.718961806659948e+17\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.49608420231201e+17\n",
      "Doing G-learning, it may take a few seconds...\n",
      "7.975048027908318e+16\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51197963381559\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51191981087625\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.5120348809883\n",
      "Doing G-learning, it may take a few seconds...\n",
      "120.51192592490837\n",
      "Doing G-learning, it may take a few seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.51185757639259\n"
     ]
    }
   ],
   "source": [
    "# L-BFGS-B for gradient solver with bounds\n",
    "res = minimize(fun, x0, method='L-BFGS-B', bounds=bnds, args=(False, 1e-6, False), options={'disp': True, 'maxiter':50}, tol=0.0000001) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "_6APDqOx-Tqx",
    "outputId": "41710a5d-050d-46fc-b135-659b64d74da2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 120.51197963381559\n",
       " hess_inv: <4x4 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ -5982.29390341,   5524.71727012,  -5370.89075432, -12205.74223857])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 125\n",
       "      nit: 7\n",
       "     njev: 25\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.91769528, 0.02445268, 1.53023955, 0.63931663])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9TcGgfihDQG1",
    "outputId": "16c1069e-f90d-443f-f897-87c7d9ea3892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.17695285e-04, 2.44526847e-01, 1.53023955e+00, 6.39316628e-01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print results. Note that these may differ from the actual G-learner parameters depending on the optimizer. \n",
    "# The optimizer will attempt to find the closest set of parameters.\n",
    "res.x/sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Wealth_Management_GIRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
